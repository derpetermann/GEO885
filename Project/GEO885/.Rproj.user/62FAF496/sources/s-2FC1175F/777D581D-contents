---
title: "Toponym patterns"
author: "Gregory Biland"
date: "12/20/2021"
output: pdf_document
---
## Setup

```{r}
knitr::opts_chunk$set(echo = FALSE)
options(stringsAsFactors = F)         # no automatic data transformation
options("scipen" = 100, "digits" = 4) # suppress math annotation

options(geonamesUsername="me_toponymboy")

names = LETTERS[1:26] ## Gives a sequence of the letters of the alphabet
beta1 = rnorm(26, 5, 2) ## A vector of slopes (one for each letter)
beta0 = 10 ## A common intercept

```


```{r}
## Gregory Biland
Sys.setenv(LANG = "en")

remotes::install_github("rlesur/klippy")
options(stringsAsFactors = F)         # no automatic data transformation
options("scipen" = 100, "digits" = 4) # suppress math annotation


# Download package Geoparser V0.1.2

require(devtools)
#install_version("geoparser", version = "0.1.2", repos = "http://cran.us.r-project.org")

# set options
# activate packages
library(rworldmap)
library(rworldxtra)
library(rjson)
library(devtools)
library(tidyverse)
library(gutenbergr)
library(DT)
library(flextable)
library(geonames)
library(dplyr)
library(geoparser)
library(here)
library(tidytext)
library(tidyr)
library(ggplot2)
library(ggmap)
library(stringr)
library(sf)
library(mapview)
library(tmap)
library(rnaturalearth)
library(rnaturalearthdata)
library(spacyr)
library(maptools)
library(rgeos)
library(entity)
library(tidygeocoder)
library(osmdata)
library(magrittr)
library(readtext)
library(raster)
library(spatstat)

spacy_initialize(model = "en_core_web_sm")

# activate klippy for copy-to-clipboard button
here("/Users/chaualala/Desktop/UZH/MSc Geographie/1. Semester/GEO 871 -  Retrieving Geographic Information/Project/pattern_analysis")
here::i_am("Toponym_patterns.R")

cities5000 <- read.delim("~/Desktop/UZH/MSc Geographie/1. Semester/GEO 871 -  Retrieving Geographic Information/Project/pattern_analysis/cities5000.txt", header=FALSE, stringsAsFactors=TRUE)
cities5000$V2 = tolower(cities5000$V2)
cities5000 <- subset(cities5000, select = -c(V7,V8,V9,V10,V11,V12,V13,V14,V15,V16,V17,V18,V19)) %>% rename("lon" = V5, "lat" = V6)

#https://cran.r-project.org/web/packages/gutenbergr/vignettes/intro.html

## {text} ----------------------------------------------------------------------------------

gutenberg_full_data <- left_join(gutenberg_works(language == "en"), gutenberg_metadata, by = "gutenberg_id")
gutenberg_full_data <- left_join(gutenberg_full_data, gutenberg_subjects)
gutenberg_full_data <- subset(gutenberg_full_data, select = -c(rights.x,has_text.x,language.y,gutenberg_bookshelf.x, gutenberg_bookshelf.y,
                                                               rights.y, has_text.y,gutenberg_bookshelf.y, gutenberg_author_id.y, title.y, author.y))
gutenberg_full_data <- gutenberg_full_data[-which(is.na(gutenberg_full_data$author.x)),]
word = c("novel", "Novel", "NOVEL")
novels <- gutenberg_full_data %>% filter(str_detect(title.x,word))

```

```{r}
# Filter 50 random books

set.seed(78)
novels_random <- sample_n(novels, 100)

original_books <- gutenberg_download((novels_random), meta_fields = "title")

original_books$text <- gsub("_", "", original_books$text)

original_books <- original_books  %>%
  group_by(title) %>%
  mutate(line = row_number()) %>%
  ungroup()

original_books #All books used in the analysis

write_csv(original_books, "/Users/chaualala/Desktop/UZH/MSc Geographie/1. Semester/GEO 871 -  Retrieving Geographic Information/Project/pattern_analysis/text.csv")

tidy_books <- original_books %>%
  unnest_tokens(word, text) 

tidy_books <- as.data.frame(unclass(tidy_books),                     # Convert all columns to factor
                            stringsAsFactors = FALSE)
tidy_books <- subset(tidy_books, select = -c(gutenberg_id))
tidy_books$row_num <- seq.int(nrow(tidy_books))


## {tidy - nouns} ----------------------------------------------------------------------------------
word_counts <- tidy_books %>%
  anti_join(stop_words, by = "word") %>%
  count(title, word, sort = TRUE) %>%
  group_by(title) %>%
  mutate(line = row_number()) %>%
  ungroup()

word_counts$word <- as.character(word_counts$word)

NER <- spacy_parse(word_counts$word, tag = TRUE, pos = TRUE)
NER <- subset(NER, pos == "PROPN" & entity == "GPE_B")
NER <- subset(NER, select = -c(sentence_id,token_id,lemma,tag) )%>% rename("word" = token)

NER <- left_join(NER,word_counts, by = "word", copy = TRUE)
NER


# {text analysis} ----------------------------------------------------------------------------------

get_sentiments("bing")

sentiment_analysis <- tidy_books %>%
  inner_join(get_sentiments("bing"), by = "word") %>% 
  count(title, index = line %/% 80, sentiment) %>% 
  spread(sentiment, n, fill = 0) %>% 
  mutate(sentiment = positive - negative)

sentiment_analysis_summary <- sentiment_analysis %>%
  group_by(title) %>%
  mutate(mean_pos = mean(positive),
         mean_neg = mean(negative),
         mean_sen = mean(sentiment))
sentiment_analysis_summary <- sentiment_analysis_summary %>% distinct(mean_neg,mean_pos,mean_sen, .keep_all= TRUE)


ggplot(sentiment_analysis_summary, aes(index, sentiment, fill = title)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  facet_wrap(~title, ncol = 2, scales = "free_x")

# {geoparsing} ----------------------------------------------------------------------------------
place_names <- left_join(NER,cities5000, by = c("word" = "V2"), copy = TRUE)

locations <- as_tibble(place_names) %>% drop_na(V3,lon,lat)
locations <- filter(locations, entity != "PERSON_B") %>% rename("Location" = V3)  %>% drop_na(title)

#add row number and tidy
locations %<>% mutate(row = row_number()) 
locations <- subset(locations, select = -c(V4,V1))

locations <- locations %>% distinct(word, .keep_all= TRUE)

locations_sf <- st_as_sf(locations, coords = c("lat", "lon"), crs = 4326)

worldmap <- st_join(world, locations_sf, join = st_contains_properly, left=TRUE)
```

```{r}
# {geocoding} ----------------------------------------------------------------------------------

newmap <- getMap(resolution = "low")

world <- ne_countries(scale = "medium", returnclass = "sf")
ggplot(data= world) +
  geom_sf(alpha = 0.7) +
  xlab("Longitude") + ylab("Latitude") +
  geom_point(data=locations, 
             aes(x=lat, y=lon, colour= title), 
             pch=20, size=4, alpha=I(0.7))+
  ggtitle("Toponym locations from english novel") + theme(legend.position = "none")

library(RColorBrewer)
palette = colorRampPalette(brewer.pal(n=96, name='Oranges'))(96)
palette = c("white", palette)
# create map
ggplot() +
  geom_sf(data = worldmap, aes(fill = Location)) +
  scale_fill_manual(values = palette) +
  # customize legend title
  labs(fill = "Population Size") +
  theme(panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank())+
  ggtitle("Toponym locations from english novel") + theme(legend.position = "none")+
  xlab("Longitude") + ylab("Latitude")

# {Analysis} ----------------------------------------------------------------------------------

paste("The effectiveness of the geoparsing is",((nrow(locations)/nrow(NER))*100),"%")

##
w1 = st_bbox(c(xmin = min(locations$lat), ymin = min(locations$lon), xmax = max(locations$lat), ymax = max(locations$lon))) %>% st_as_sfc()
pp1 = c(w1, st_geometry(locations_sf)) %>% as.ppp()


####

Q <- quadratcount(pp1, nx= 10, ny= 5)
hist(Q, main=NULL, las = 1, freq = TRUE)

cols <- "grey90"

plot(newmap, col = alpha(cols,0.2), main = "Locations per grid cell from english novels")  # Plot map
points(locations$lat, locations$lon,col = "red", cex = .8, pch = 20)
plot(Q, add=TRUE, lwd = 2) # Add quadrat grid

Q.d <- intensity(Q)

Q.d <- gIntersection(newmap, Q.d, drop_lower_td = TRUE) #clip polygon 2 with polygon 1

# Plot the density
plot(newmap,cols = "grey70", main = "Density of points within each quadrat")  # Plot map
plot(intensity(Q, image=TRUE), main=NULL, las=1, add = TRUE, alpha = 0.5)  # Plot density raster
points(locations$lat, locations$lon,col = "red", cex = .1)

##
locations_crd <- st_coordinates(locations_sf)

lof <- dbscan::lof(locations_crd, minPts = 4)
lof

plot(locations_crd, pch = ".", main = "LOF (k = 4)")
plot(newmap, add= TRUE, lwd =.3 )  # Plot map
points(locations_crd, cex = (lof - 1) * 3, pch = 1, col = "red")

##
ann.p <- mean(nndist(locations, k=1))
ann.p

plot(locations_model1)
```

```{r}
### [Output]

st_write(locations_sf, "/Users/chaualala/Desktop/UZH/MSc Geographie/1. Semester/GEO 871 -  Retrieving Geographic Information/Project/pattern_analysis/locations.shp")
write_csv(locations, "/Users/chaualala/Desktop/UZH/MSc Geographie/1. Semester/GEO 871 -  Retrieving Geographic Information/Project/pattern_analysis/locations.csv")
```

